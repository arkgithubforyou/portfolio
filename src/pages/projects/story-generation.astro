---
import ProjectLayout from '../../layouts/ProjectLayout.astro';

const project = {
  title: "Controlled Story Generation",
  description: "Multi-task neural architecture for controlled text generation with rich details.",
  tags: ["PyTorch", "Text Generation", "Attention", "Crowdsourcing"],
  github: "",
  demo: "",
};

const baseUrl = import.meta.env.BASE_URL;
---

<ProjectLayout {...project}>
  <p class="text-lg text-slate-600 mb-6">
    <a href="https://aclanthology.org/2020.coling-main.212/" target="_blank" rel="noopener noreferrer" class="text-primary-600 hover:underline">Published at COLING 2020</a>
  </p>

  <img src={`${baseUrl}images/story0.png`} alt="Story Generation Architecture" class="rounded-lg mb-8 w-full" />

  <h2>Overview</h2>
  <p>
    Automatically generated text should be both coherent and engaging, but most language models face a fundamental trade-off: they either produce generic, boring content or lose coherence when trying to add detail. This project solved this problem by developing a production-ready neural architecture that generates engaging, detailed narratives while maintaining global coherence.
  </p>

  <h2>Impact</h2>
  <ul>
    <li><strong>34% improvement</strong> in content informativeness over state-of-the-art baseline</li>
    <li><strong>90% data reduction</strong> through strategic use of pre-trained embeddings</li>
    <li>Maintained global coherence (0.49 vs 0.40 baseline)</li>
    <li>Production speed: 8 tokens/second on single GPU</li>
    <li>Results validated through rigorous human evaluation (200 stories, 1,221 ratings)</li>
  </ul>

  <h2>Technical Approach</h2>
  <p>
    The system uses a multi-task learning framework with three key components:
  </p>
  <ul>
    <li><strong>Agenda-aware encoder:</strong> Bi-LSTM that tracks generation progress through the planned narrative structure</li>
    <li><strong>Dual decoders:</strong> Separate decoders for structural content (main events) and enrichment content (engaging details)</li>
    <li><strong>Maximum Mutual Information objective:</strong> Anti-generic decoding that penalizes common phrases and promotes specific, contextually relevant content</li>
  </ul>

  <h2>Key Challenges Solved</h2>
  <h3>Evaluating Generated Story Quality</h3>
  <p>
    Standard metrics like BLEU correlate poorly with human judgments of narrative quality. We designed a comprehensive human evaluation schema across multiple dimensions, implemented via LingoTurk on Prolific with 10 native speakers rating each story.
  </p>

  <h3>Reducing Generic Text Generation</h3>
  <p>
    Neural language models tend to produce generic, high-probability text. We implemented MMI decoding that actively suppresses high-frequency generic phrases:
  </p>
  <pre class="bg-slate-100 p-4 rounded-lg overflow-x-auto"><code>score = log p(text|context) - 0.1 Ã— log p(text)</code></pre>

  <h2>Results</h2>
  <table class="w-full text-left border-collapse mb-6">
    <thead>
      <tr class="border-b border-slate-200">
        <th class="py-2">Model</th>
        <th class="py-2">Informativeness</th>
        <th class="py-2">Global Coherence</th>
      </tr>
    </thead>
    <tbody>
      <tr class="border-b border-slate-100">
        <td class="py-2">Baseline</td>
        <td class="py-2">0.38</td>
        <td class="py-2">0.40</td>
      </tr>
      <tr class="border-b border-slate-100">
        <td class="py-2 font-semibold">Our Model</td>
        <td class="py-2 font-semibold">0.51</td>
        <td class="py-2 font-semibold">0.49</td>
      </tr>
      <tr>
        <td class="py-2 text-slate-500">Human (upper bound)</td>
        <td class="py-2 text-slate-500">0.66</td>
        <td class="py-2 text-slate-500">0.66</td>
      </tr>
    </tbody>
  </table>
</ProjectLayout>

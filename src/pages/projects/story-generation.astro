---
import ProjectLayout from '../../layouts/ProjectLayout.astro';

const project = {
  title: "Controlled Story Generation",
  description: "Multi-task neural architecture for controlled text generation with rich details.",
  tags: ["PyTorch", "Text Generation", "Attention", "Crowdsourcing"],
};

const baseUrl = import.meta.env.BASE_URL;
---

<ProjectLayout {...project}>
  <p>
    <a href="https://aclanthology.org/2020.coling-main.212/" target="_blank" rel="noopener noreferrer">Published at COLING 2020</a>
  </p>

  <img src={`${baseUrl}images/story0.png`} alt="Story Generation Architecture" />

  <h2>Overview</h2>
  <p>
    Automatically generated text should be both coherent and engaging, but most language models face a fundamental trade-off: they either produce generic, boring content or lose coherence when trying to add detail. This project solved this problem by developing a production-ready neural architecture that generates engaging, detailed narratives while maintaining global coherence. We also developed evaluation infrastructure for comprehensive human evaluation pipeline using LingoTurk and Prolific, managing 400+ crowdsourcing sessions with statistical quality control and inter-rater reliability analysis.
  </p>

  <h2>Impact</h2>
  <ul>
    <li><strong>34% improvement</strong> in content informativeness over state-of-the-art baseline</li>
    <li><strong>90% data reduction</strong> through strategic use of pre-trained embeddings</li>
    <li>Maintained global coherence (0.49 vs 0.40 baseline)</li>
    <li>Production speed: 8 tokens/second on single GPU</li>
    <li>Results validated through rigorous human evaluation (200 stories, 1,221 ratings)</li>
  </ul>

  <h2>Approach</h2>
  <p>The system uses a multi-task learning framework with three key components:</p>
  <ul>
    <li><strong>Agenda-aware encoder:</strong> Bi-LSTM that tracks generation progress through the planned narrative structure</li>
    <li><strong>Dual decoders:</strong> Separate decoders for structural content (main events) and enrichment content (engaging details)</li>
    <li><strong>Maximum Mutual Information objective:</strong> Anti-generic decoding that penalizes common phrases and promotes specific, contextually relevant content</li>
  </ul>

  <p>The human evaluation pipeline</p>
  <ul>
    <li><strong>Experimental design:</strong> A multi-criteria evaluation framework covering syntax, global coherence (inclusion, order, relevance), local coherence, and informativeness using 1-4 Likert scales, plus agenda coverage assessment</li>
    <li><strong>Platform development:</strong> Implemented the evaluation interface using LingoTurk, a custom psycholinguistics crowdsourcing platform</li>
    <li><strong>Quality control pipeline:</strong> 
	<ul>
		<li>Piloted evaluation protocol to refine questions and ensure clarity.</li>
		<li>Implemented automatic filtering of incomplete responses and statistical outliers (>3 standard deviations).</li>
		<li>Restricted to native English speakers only.</li>
		<li>Collected 10+ independent ratings per story for statistical reliability.</li>
	</ul>
	


</li>
<li><strong>Data collection management:</strong> 
	<ul>
		<li>Recruited and managed ~400 participant sessions via Prolific, with appropriate compensation (£1.5 per task, ~£7/hour).</li>
		<li>Evaluated 200 stories total (4 stories × 10 scenarios × 5 systems), resulting in 1,221 valid evaluation responses after quality filtering.</li>
	</ul>
	 </li>
<li><strong>Statistical validation:</strong> Performed inter-rater reliability analysis (Fleiss' kappa = 0.34 for agenda coverage), paired t-tests for significance testing (α = 0.05), and correlation analysis with automatic metrics</li>
  </ul>

	

  <h2>Key Challenges Solved</h2>
  <h3>Evaluating Generated Story Quality</h3>
  <p>
    Standard metrics like BLEU correlate poorly with human judgments of narrative quality. We designed a comprehensive human evaluation schema across multiple dimensions, implemented via LingoTurk on Prolific with 10 native speakers rating each story.
  </p>

  <h3>Reducing Generic Text Generation</h3>
  <p>Neural language models tend to produce generic, high-probability text. We implemented MMI decoding that actively suppresses high-frequency generic phrases:</p>
  <pre><code>score = log p(text|context) - 0.1 × log p(text)</code></pre>

  <h2>Results</h2>
  <table>
    <thead>
      <tr>
        <th>Model</th>
        <th>Informativeness</th>
        <th>Global Coherence</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Baseline</td>
        <td>0.38</td>
        <td>0.40</td>
      </tr>
      <tr>
        <td><strong>Our Model</strong></td>
        <td><strong>0.51</strong></td>
        <td><strong>0.49</strong></td>
      </tr>
      <tr>
        <td>Human (upper bound)</td>
        <td>0.66</td>
        <td>0.66</td>
      </tr>
    </tbody>
  </table>
</ProjectLayout>

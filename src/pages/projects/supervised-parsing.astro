---
import ProjectLayout from '../../layouts/ProjectLayout.astro';

const project = {
  title: "Supervised Script Parsing",
  description: "Hierarchical sequence modeling for procedural knowledge extraction.",
  tags: ["NLU", "XLNet", "Back-translation", "Sequence Labeling"],
  github: "",
  demo: "",
};

const baseUrl = import.meta.env.BASE_URL;
---

<ProjectLayout {...project}>
  <p class="text-lg text-slate-600 mb-6">
    <a href="https://aclanthology.org/2021.starsem-1.18" target="_blank" rel="noopener noreferrer" class="text-primary-600 hover:underline">Published at *SEM 2021</a>
  </p>

  <img src={`${baseUrl}images/ssp.png`} alt="Supervised Script Parsing Architecture" class="rounded-lg mb-8 w-full" />

  <h2>Overview</h2>
  <p>
    Understanding how everyday activities unfold is fundamental to many NLP applications, from conversational AI to content generation. Script parsing is the task of automatically identifying which parts of a text correspond to activity-specific events and participants.
  </p>
  <p>
    This task is challenging even for humans (inter-annotator agreement: 0.64-0.77 Fleiss' κ). Previous approaches achieved only 66% accuracy on event parsing and had not successfully addressed participant parsing at all.
  </p>

  <h2>Impact</h2>
  <ul>
    <li><strong>+16.1 F1 points</strong> improvement over previous SOTA in event parsing</li>
    <li><strong>85.7% micro-F1</strong> for event parsing</li>
    <li><strong>90.3% micro-F1</strong> for participant parsing (first successful system)</li>
    <li>Single scenario-agnostic model handles all 10 activity scenarios</li>
  </ul>

  <h2>Technical Approach</h2>
  <p>
    The key insight is that script parsing requires information at two different granularities:
  </p>
  <ul>
    <li><strong>Word-level patterns:</strong> How events are expressed in surface language</li>
    <li><strong>Discourse-level patterns:</strong> The procedural order in which events occur</li>
  </ul>

  <h3>Architecture</h3>
  <p>Two-level sequence modeling:</p>
  <ul>
    <li><strong>Stage 1 - Word sequence model:</strong> XLNet-base-cased encoding of the full story</li>
    <li><strong>Stage 2 - Event sequence model:</strong> Bi-LSTM over extracted candidates, modeling discourse-level patterns</li>
  </ul>

  <h3>Addressing Data Sparsity</h3>
  <p>With 26 classes having fewer than 10 instances:</p>
  <ul>
    <li><strong>Domain adaptation:</strong> Corpus embeddings to handle cross-corpus differences</li>
    <li><strong>Back-translation augmentation:</strong> English → French → English paraphrasing via Google Translate</li>
  </ul>

  <h2>Results</h2>
  <table class="w-full text-left border-collapse mb-6">
    <thead>
      <tr class="border-b border-slate-200">
        <th class="py-2">Model</th>
        <th class="py-2">Event F1</th>
        <th class="py-2">Participant F1</th>
      </tr>
    </thead>
    <tbody>
      <tr class="border-b border-slate-100">
        <td class="py-2">Previous SOTA (CRF)</td>
        <td class="py-2">58.1 / 66.0</td>
        <td class="py-2">n/a</td>
      </tr>
      <tr class="border-b border-slate-100">
        <td class="py-2">Fine-tuned XLNet</td>
        <td class="py-2">62.1 / 79.3</td>
        <td class="py-2">79.7 / 77.2</td>
      </tr>
      <tr>
        <td class="py-2 font-semibold">Our Full Model</td>
        <td class="py-2 font-semibold">75.1 / 85.7</td>
        <td class="py-2 font-semibold">80.3 / 90.3</td>
      </tr>
    </tbody>
  </table>
  <p class="text-sm text-slate-500">Values shown as macro / micro F1</p>

  <h2>Key Findings</h2>
  <p>
    Ablation study removing the event sequence model dropped performance from 70.1 to 63.3 macro-F1, confirming that discourse-level modeling is essential. Error analysis showed 23% of errors were actually annotation errors in the original data.
  </p>
</ProjectLayout>

---
import ProjectLayout from '../../layouts/ProjectLayout.astro';

const project = {
  title: "Gregg Shorthand Recognition",
  description: "Deep learning system for recognizing English words from Gregg shorthand images.",
  tags: ["Image to Text", "CNN/LSTM", "Retrieval", "Dataset Construction"],
  github: "",
  demo: "",
};

const baseUrl = import.meta.env.BASE_URL;
---

<ProjectLayout {...project}>
  <p class="text-lg text-slate-600 mb-6">
    <a href="https://link.springer.com/chapter/10.1007/978-3-030-00794-2_24" target="_blank" rel="noopener noreferrer" class="text-primary-600 hover:underline">Published at TSD 2018</a>
  </p>

  <img src={`${baseUrl}images/gregg1.png`} alt="Gregg Shorthand Recognition Pipeline" class="rounded-lg mb-8 w-full" />

  <h2>Overview</h2>
  <p>
    Gregg shorthand is the most widely used form of pen stenography in the United States, capable of reaching writing speeds of 200 words per minute. Despite the prevalence of electronic devices, shorthand remains in use today due to its unique advantages.
  </p>
  <p>
    Automatically recognizing these hand-written scripts presents significant technical challenges due to their extremely concise and subtle visual encoding. This project developed the <strong>first substantial corpus for optical Gregg shorthand recognition</strong> and created a novel neural architecture achieving 58% top-5 accuracy on a vocabulary of over 15,000 English words.
  </p>

  <h2>Impact</h2>
  <ul>
    <li>Created <strong>Gregg-1916 corpus</strong>: 15,711 shorthand images with English word labels</li>
    <li><strong>34.9% top-1 accuracy</strong> on 15K+ word vocabulary</li>
    <li><strong>58.0% top-5 accuracy</strong></li>
    <li>First publicly available corpus for optical shorthand recognition</li>
  </ul>

  <h2>Key Challenge</h2>
  <p>
    Shorthand encodes multiple characters in tiny regions through subtle variations in stroke direction, size, and orientation. Images vary dramatically in size (smallest ~1% the size of largest) and cannot be augmented through standard rotation or scaling without losing critical information.
  </p>

  <h2>Technical Approach</h2>
  <p>
    The key innovation is reformulating word-level shorthand recognition as a <strong>hybrid task</strong>: generating character sequences from visual features, then retrieving the best-matching word from a vocabulary.
  </p>

  <h3>Three-Stage Pipeline</h3>
  <ol>
    <li><strong>Feature extraction:</strong> 10-layer CNN with batch normalization extracts visual features from variable-sized shorthand images</li>
    <li><strong>Bidirectional sequence generation:</strong> Two recurrent networks (forward and backward) generate character sequences, addressing performance decay across the sequence</li>
    <li><strong>Word retrieval:</strong> Custom retrieval module ranks vocabulary words using weighted combinations of Levenshtein distance and a novel bi-directional BLEU score</li>
  </ol>

  <h3>Why Bidirectional?</h3>
  <p>
    Single-direction decoders exhibit severe performance decay: accuracy drops from 95% for the first character to 64% average. The backward decoder provides reliable predictions for word endings, while the forward decoder excels at beginnings.
  </p>

  <h2>Results</h2>
  <table class="w-full text-left border-collapse mb-6">
    <thead>
      <tr class="border-b border-slate-200">
        <th class="py-2">Metric</th>
        <th class="py-2">Score</th>
      </tr>
    </thead>
    <tbody>
      <tr class="border-b border-slate-100">
        <td class="py-2">Top-1 Retrieval Accuracy</td>
        <td class="py-2">34.9%</td>
      </tr>
      <tr class="border-b border-slate-100">
        <td class="py-2">Top-5 Retrieval Accuracy</td>
        <td class="py-2">58.0%</td>
      </tr>
      <tr class="border-b border-slate-100">
        <td class="py-2">Editorial Similarity</td>
        <td class="py-2">64.4%</td>
      </tr>
      <tr>
        <td class="py-2">Raw Sequence Generation</td>
        <td class="py-2">2.7%</td>
      </tr>
    </tbody>
  </table>
  <p>
    The retrieval module proved essentialâ€”raw sequence generation achieved only 2.7% accuracy, demonstrating that the hybrid approach successfully bridges the gap between imperfect sequence generation and correct word identification.
  </p>

  <h2>Key Findings</h2>
  <p>
    Error analysis showed that retrieval failures often involved words with similar pronunciation patterns. Since shorthand is designed to encode pronunciation rather than spelling, incorporating phonetic information could substantially improve future performance.
  </p>
</ProjectLayout>
